{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YIL-GPT — GenAI & RAG Project\n",
    "\n",
    "This notebook represents a multi-module project flattened into a single `.ipynb`.\n",
    "Each section below corresponds to a Python module (e.g. `config.py`, `ingestion.py`, etc.).\n",
    "You can either run cells directly here or export them back into separate `.py` files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"config.py — Centralized configuration for YIL-GPT (GenAI + RAG system).\"\"\"\n",
    "\n",
    "# Embedding & LLM configuration\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\" \n",
    "LLM_MODEL = \"gpt-4o-mini\"  # placeholder for the main LLM\n",
    "\n",
    "CONTEXT_WINDOW = 4096\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 120\n",
    "\n",
    "# Vector DB configuration\n",
    "VECTOR_DB_PATH = \"data/vdb\"\n",
    "INDEX_NAME = \"yilgpt_index\"\n",
    "TOP_K = 5\n",
    "\n",
    "# Document paths\n",
    "DATA_ROOT = \"documents\"\n",
    "MANUALS_DIR = f\"{DATA_ROOT}/manuals\"\n",
    "SOPS_DIR = f\"{DATA_ROOT}/sops\"\n",
    "LOGS_DIR = f\"{DATA_ROOT}/alarm_logs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils/text_cleaning.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Utility functions for text cleaning and normalization.\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"Collapse multiple spaces/newlines into a single space.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Basic cleaning used before chunking and embedding.\n",
    "\n",
    "    This is intentionally simple; extend with domain-specific rules if needed.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\u00a0\", \" \")  # non‑breaking spaces\n",
    "    text = normalize_whitespace(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils/pdf_reader.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"PDF / file reading helpers.\n",
    "\n",
    "In a real project you might use:\n",
    "- pdfplumber\n",
    "- pymupdf\n",
    "- pypdf\n",
    "\n",
    "Here we only stub the interface so the rest of the pipeline is clear.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def extract_pdf_text(path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file.\n",
    "\n",
    "    This is a placeholder implementation. Replace with a real extractor\n",
    "    such as pdfplumber when running in production.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    # TODO: implement real PDF parsing\n",
    "    return p.read_text(errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils/logger.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Simple logging wrapper used across the project.\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "LOGGER_NAME = \"yil_gpt\"\n",
    "\n",
    "\n",
    "def get_logger(name: str = LOGGER_NAME) -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        fmt = logging.Formatter(\"[%(asctime)s] [%(levelname)s] %(message)s\")\n",
    "        handler.setFormatter(fmt)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ingestion.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Document ingestion and chunking logic for YIL-GPT.\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.text_cleaning import clean_text\n",
    "from utils.pdf_reader import extract_pdf_text\n",
    "from utils.logger import get_logger\n",
    "from config import CHUNK_SIZE, CHUNK_OVERLAP\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "def load_document(path: str) -> str:\n",
    "    \"\"\"Load a document. Uses PDF extractor as a default.\n",
    "\n",
    "    You can extend this with logic for `.txt`, `.docx`, etc.\n",
    "    \"\"\"\n",
    "    path_obj = Path(path)\n",
    "    if path_obj.suffix.lower() == \".pdf\":\n",
    "        text = extract_pdf_text(path)\n",
    "    else:\n",
    "        text = path_obj.read_text(errors=\"ignore\")\n",
    "    return clean_text(text)\n",
    "\n",
    "\n",
    "def create_chunks(text: str, metadata: Dict) -> List[Dict]:\n",
    "    \"\"\"Hybrid chunking strategy.\n",
    "\n",
    "    - Uses a simple word-based sliding window\n",
    "    - Chunk size & overlap controlled via config\n",
    "    - Each chunk carries the original document metadata\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks: List[Dict] = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + CHUNK_SIZE\n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        if not chunk_text.strip():\n",
    "            break\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"metadata\": metadata.copy(),\n",
    "        })\n",
    "        start += max(CHUNK_SIZE - CHUNK_OVERLAP, 1)\n",
    "    logger.info(\"Created %d chunks\", len(chunks))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vector_db.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"In-memory Vector DB stub for YIL-GPT.\n",
    "\n",
    "This is intentionally simple so the notebook is self‑contained.\n",
    "In production you would replace this with FAISS, Chroma, Qdrant, Weaviate, etc.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from config import TOP_K\n",
    "from utils.logger import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "# --- Very simple embedding stub -------------------------------------------------\n",
    "\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    \"\"\"Dummy embedding function.\n",
    "\n",
    "    Replace this with a real embedding model (e.g. SentenceTransformers).\n",
    "    For now it just hashes text into a fixed‑size vector for demonstration.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(abs(hash(text)) % (2**32))\n",
    "    return rng.normal(size=384).astype(\"float32\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VectorDB:\n",
    "    \"\"\"A toy Vector DB with linear search.\n",
    "\n",
    "    This keeps the code easy to run in a notebook while still showing\n",
    "    clearly how retrieval works conceptually.\n",
    "    \"\"\"\n",
    "\n",
    "    vectors: List[np.ndarray] = field(default_factory=list)\n",
    "    docs: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "    def add_documents(self, docs: List[Dict[str, Any]]) -> None:\n",
    "        for doc in docs:\n",
    "            vec = embed_text(doc[\"text\"])\n",
    "            self.vectors.append(vec)\n",
    "            self.docs.append(doc)\n",
    "        logger.info(\"Indexed %d documents (total=%d)\", len(docs), len(self.docs))\n",
    "\n",
    "    def search(self, query: str, top_k: Optional[int] = None, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "        if top_k is None:\n",
    "            top_k = TOP_K\n",
    "        q_vec = embed_text(query)\n",
    "\n",
    "        # Apply simple metadata filtering\n",
    "        candidate_indices = list(range(len(self.docs)))\n",
    "        if filters:\n",
    "            def match(meta, filters):\n",
    "                return all(meta.get(k) == v for k, v in filters.items())\n",
    "            candidate_indices = [i for i in candidate_indices if match(self.docs[i][\"metadata\"], filters)]\n",
    "\n",
    "        if not candidate_indices:\n",
    "            return []\n",
    "\n",
    "        # Cosine similarity\n",
    "        mat = np.stack([self.vectors[i] for i in candidate_indices])\n",
    "        q_norm = q_vec / (np.linalg.norm(q_vec) + 1e-9)\n",
    "        mat_norm = mat / (np.linalg.norm(mat, axis=1, keepdims=True) + 1e-9)\n",
    "        sims = mat_norm @ q_norm\n",
    "\n",
    "        top_idx = np.argsort(-sims)[:top_k]\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for rank, idx in enumerate(top_idx):\n",
    "            doc_idx = candidate_indices[int(idx)]\n",
    "            d = self.docs[doc_idx].copy()\n",
    "            d[\"score\"] = float(sims[int(idx)])\n",
    "            d[\"rank\"] = int(rank)\n",
    "            results.append(d)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm_client.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"LLM client wrapper used for query rewriting and answer generation.\n",
    "\n",
    "This uses placeholder logic so that the notebook is runnable without real API keys.\n",
    "Replace `rewrite_query` and `generate_answer` with actual LLM calls in production.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "from utils.logger import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, model_name: str = None):\n",
    "        self.model_name = model_name or \"gpt-4o-mini\"\n",
    "\n",
    "    def rewrite_query(self, query: str) -> str:\n",
    "        \"\"\"Lightweight query rewriting stub.\n",
    "\n",
    "        In production, this would call an LLM to:\n",
    "        - infer system/unit\n",
    "        - normalize terminology\n",
    "        - expand abbreviations\n",
    "        \"\"\"\n",
    "        logger.info(\"Rewriting query: %s\", query)\n",
    "        return query.strip()\n",
    "\n",
    "    def generate_answer(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
    "        \"\"\"Generate an answer from retrieved docs.\n",
    "\n",
    "        For demonstration, we simply concatenate top documents.\n",
    "        In production this would call an LLM with a RAG prompt template.\n",
    "        \"\"\"\n",
    "        logger.info(\"Generating answer for query with %d context docs\", len(retrieved_docs))\n",
    "        context_summary = \"\\n\\n\".join(\n",
    "            f\"[Doc {i} | score={d.get('score', 0):.3f}]\\n{d['text'][:400]}...\" for i, d in enumerate(retrieved_docs)\n",
    "        )\n",
    "        answer = (\n",
    "            \"YIL-GPT (stub) answer\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            \"Context used (top documents):\\n\"\n",
    "            f\"{context_summary}\"\n",
    "        )\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieval.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Retrieval pipeline for YIL-GPT.\n",
    "\n",
    "Implements:\n",
    "- Query rewriting\n",
    "- Metadata-aware vector search\n",
    "- Returns top-k documents for RAG\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from vector_db import VectorDB\n",
    "from llm_client import LLMClient\n",
    "from utils.logger import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, vector_db: VectorDB | None = None, llm: LLMClient | None = None):\n",
    "        self.vdb = vector_db or VectorDB()\n",
    "        self.llm = llm or LLMClient()\n",
    "\n",
    "    def retrieve(self, query: str, filters: Dict | None = None) -> List[Dict]:\n",
    "        rewritten = self.llm.rewrite_query(query)\n",
    "        logger.info(\"Running retrieval with filters=%s\", filters)\n",
    "        results = self.vdb.search(rewritten, filters=filters)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rag_pipeline.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"High-level RAG pipeline for YIL-GPT.\"\"\"\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from llm_client import LLMClient\n",
    "from retrieval import Retriever\n",
    "from vector_db import VectorDB\n",
    "\n",
    "\n",
    "class YILGPT:\n",
    "    \"\"\"Facade over the full RAG stack.\n",
    "\n",
    "    Usage:\n",
    "        bot = YILGPT()\n",
    "        answer = bot.query(\"Why did compressor C-101 trip yesterday?\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_db: VectorDB | None = None):\n",
    "        self.vdb = vector_db or VectorDB()\n",
    "        self.llm = LLMClient()\n",
    "        self.retriever = Retriever(self.vdb, self.llm)\n",
    "\n",
    "    def query(self, user_query: str, filters: Dict | None = None) -> str:\n",
    "        retrieved = self.retriever.retrieve(user_query, filters=filters)\n",
    "        answer = self.llm.generate_answer(user_query, retrieved)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# api.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Simple FastAPI wrapper exposing YIL-GPT as an HTTP API.\n",
    "\n",
    "To run (after installing fastapi + uvicorn):\n",
    "\n",
    "    uvicorn api:app --reload\n",
    "\n",
    "Then send POST requests to /query with JSON: {\"query\": \"...\"}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    from fastapi import FastAPI\n",
    "    from pydantic import BaseModel\n",
    "\n",
    "    from rag_pipeline import YILGPT\n",
    "\n",
    "    app = FastAPI(title=\"YIL-GPT API\")\n",
    "    bot = YILGPT()\n",
    "\n",
    "    class QueryRequest(BaseModel):\n",
    "        query: str\n",
    "\n",
    "\n",
    "    @app.post(\"/query\")\n",
    "    def query(req: QueryRequest):\n",
    "        answer = bot.query(req.query)\n",
    "        return {\"answer\": answer}\n",
    "\n",
    "except Exception as e:  # FastAPI might not be installed in the notebook env\n",
    "    # This allows the rest of the notebook to run without FastAPI.\n",
    "    print(\"FastAPI not available or failed to import:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README (project-level)\n",
    "\n",
    "This notebook corresponds to a modular GenAI + RAG project called **YIL-GPT**.\n",
    "In a real repo these sections would live in separate `.py` files.\n",
    "\n",
    "Key ideas covered:\n",
    "- Chunking strategy for long industrial manuals and logs\n",
    "- Embedding + Vector DB for semantic search\n",
    "- Retrieval pipeline (rewrite → filter → search)\n",
    "- RAG answer generation and hallucination-aware prompting\n",
    "- Optional FastAPI layer for serving the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
